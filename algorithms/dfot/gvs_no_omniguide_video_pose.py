from typing import Optional
from jaxtyping import Float
from omegaconf import DictConfig
import torch
from torch import Tensor
from einops import rearrange, repeat
from tqdm import tqdm
from .history_guidance import HistoryGuidance, GeneralizedHistoryGuidanceManager
from .dfot_video_pose import DFoTVideoPose


class GVSNoOmniGuideVideoPose(DFoTVideoPose):
    """
    A child class of DFoTVideoPose that implements Generative View Stitching (GVS) with no omniguidance i.e. no guidance on neighboring chunks -- just guidance on external conditioning such as camera poses.
    """

    def __init__(self, cfg: DictConfig):
        super().__init__(cfg)

    def _predict_videos(
        self,
        xs: Float[Tensor, "B T C H W"],
        conditions: Optional[Float[Tensor, "B T ..."]] = None,
    ) -> Tensor:

        # 1) initialize chunks with Gaussian noise, unless they are the context frames
        # note that xs, conditions are of shape (B, T, ...), where xs contains all ground truth frames and we just want to condition on the first n_context_frames as context
        batch_size = xs.shape[0]
        target_length = xs.shape[1]
        chunk_sizes = self.cfg.tasks.prediction.chunk_sizes

        assert self.max_tokens == sum(
            chunk_sizes
        ), "max_tokens must be equal to the sum of chunk_sizes"
        chunk_to_denoise_size = chunk_sizes[1]
        assert (
            target_length % chunk_to_denoise_size == 0
        ), "target_length must be divisible by chunk_to_denoise_size"
        chunk_sizes = torch.tensor(chunk_sizes, device=self.device)

        xs_pred = torch.randn(
            (
                xs.shape[0],
                xs.shape[1] + chunk_sizes[0] + chunk_sizes[-1],
                *self.x_shape,
            ),
            device=self.device,
            generator=self.generator,
        )
        xs_pred = torch.clamp(xs_pred, -self.clip_noise, self.clip_noise)
        xs_pred[:, chunk_sizes[0] : chunk_sizes[0] + self.n_context_tokens] = xs[
            :, : self.n_context_tokens
        ].clone()  # for the first n_context_tokens, we use ground truth frames as context and for the remaining frames, we initialize with Gaussian noise
        # xs_pred[:, -self.n_context_tokens:] = xs[:, -self.n_context_tokens:].clone()                                # for the last n_context_tokens, we use ground truth frames as context

        # extend conditions
        conditions = torch.cat(
            [
                conditions[:, : chunk_sizes[0]],
                conditions,
                conditions[:, -chunk_sizes[-1] :],
            ],
            dim=1,
        )

        # a binary mask that indicates which tokens are context (either ground-truth or fully denoised) and which ones need to be denoised
        # possible values:
        # -1: padding
        # 0: to be denoised
        # 1: ground-truth context
        # 2: fully denoised
        context_mask = torch.zeros(xs_pred.shape[:2], device=self.device)
        context_mask[:, chunk_sizes[0] : chunk_sizes[0] + self.n_context_tokens] = 1

        # left-most padding chunk and right-most padding chunk are marked by -1
        context_mask[:, : chunk_sizes[0]] = -1
        context_mask[:, -chunk_sizes[-1] :] = -1

        # 2) create chunk triplets: (prefix_chunk, chunk_to_denoise, suffix_chunk)
        #    there are two types of chunk triplets:
        #    a) chunk triplets generated by sliding the window chunk_length by chunk_length
        #       i.e. (chunk_t-1, chunk_t, chunk_t+1)
        #    b) chunk triplets manually created by user e.g. those based on heuristics on pose similarity to "close" the loop or allow information propagation between chunks with similar but not identical conditions

        # create an array of indices that will index into xs_pred and conditions to create chunk triplets
        # the array is of shape (B, num_windows, self.max_tokens)
        # first dimension = number of batches
        # second dimension = number of sliding windows
        # third dimension = length of the sliding window
        scheduling_matrix = self._generate_scheduling_matrix(
            xs_pred.shape[1]
        )  # shape = (num_denoising_steps, target_length)
        scheduling_matrix = scheduling_matrix.to(self.device)

        # shape = (num_chunks_to_denoise, self.max_tokens)
        start_indices = torch.arange(
            0,
            xs_pred.shape[1] - self.max_tokens + 1,
            chunk_to_denoise_size,
            device=self.device,
        )
        indices_to_cycle_through_per_denoising_step = start_indices[
            :, None
        ] + torch.arange(self.max_tokens, device=self.device)

        # create an array of indices that will index into xs_pred and conditions to create chunk triplets
        # the array is of shape (B, num_windows, self.max_tokens)
        # first dimension = number of batches
        # second dimension = number of sliding windows
        # third dimension = length of the sliding window
        scheduling_matrix = self._generate_scheduling_matrix(
            xs_pred.shape[1]
        )  # shape = (num_denoising_steps, target_length)
        scheduling_matrix = scheduling_matrix.to(self.device)
        num_denoising_steps = scheduling_matrix.shape[0]

        # shape = (num_chunks_to_denoise, self.max_tokens)
        start_indices = torch.arange(
            0,
            xs_pred.shape[1] - self.max_tokens + 1,
            chunk_to_denoise_size,
            device=self.device,
        )
        temporal_window_indices = start_indices[:, None] + torch.arange(
            self.max_tokens, device=self.device
        )

        # add manual windows comprised of frame indices with temporal windows
        # total_window_indices.shape = (num_windows, self.max_tokens)
        if self.cfg.tasks.prediction.manual_window_indices is not None:
            manual_window_indices = torch.tensor(
                self.cfg.tasks.prediction.manual_window_indices,
                device=self.device,
            )
            manual_window_indices = repeat(
                manual_window_indices,
                "num_manual_windows max_tokens -> (num_repeat num_manual_windows) max_tokens",
                num_repeat=self.cfg.tasks.prediction.repeat_manual_window_indices,
            )
            # shift the indices by chunk_sizes[0] to account for the padding chunks
            manual_window_indices = manual_window_indices + chunk_sizes[0]

            # concat with temporal windows
            total_window_indices = torch.cat(
                [temporal_window_indices, manual_window_indices], dim=0
            )  # shape = (num_windows = num_chunks_to_denoise + num_manual_windows, self.max_tokens)
        else:
            total_window_indices = temporal_window_indices

        # for each chunk-to-denoise, create a list of window indices (i.e. that indexes into "total_window_indices") to cycle through

        windows_to_cycle_through = {}
        for i, temporal_window in enumerate(temporal_window_indices):
            chunk_to_denoise = temporal_window[chunk_sizes[0] : -chunk_sizes[-1]]

            # find windows inside "total_window_indices" that contain the chunk-to-denoise as its central chunk
            windows = torch.where(
                (
                    total_window_indices[:, chunk_sizes[0] : -chunk_sizes[-1]]
                    == chunk_to_denoise
                ).all(dim=1)
            )[0]
            windows_to_cycle_through[i] = windows

        # shape = (num_chunks_to_denoise,)
        num_windows_to_cycle_through = torch.tensor(
            [len(windows) for windows in windows_to_cycle_through.values()],
            device=self.device,
        )

        # shape = (num_denoising_steps, num_chunks_to_denoise)
        # contains indices into windows_to_cycle_through[chunk_to_denoise]
        index_into_windows_to_cycle_through_per_denoising_step = torch.remainder(
            torch.arange(num_denoising_steps, device=self.device)[:, None],
            num_windows_to_cycle_through[None],
        )

        # shape = (num_denoising_steps, num_chunks_to_denoise)
        # these window indices will be used to index into chunk_indices, which has shape (num_windows, chunk_triplets)
        window_indices_to_cycle_through_per_denoising_step = torch.stack(
            [
                windows_to_cycle_through[i][
                    index_into_windows_to_cycle_through_per_denoising_step[:, i]
                ]
                for i in range(temporal_window_indices.shape[0])
            ],
            dim=1,
        )

        # shape = (num_denoising_steps, num_chunks_to_denoise, self.max_tokens)
        indices_to_cycle_through_per_denoising_step = total_window_indices[
            window_indices_to_cycle_through_per_denoising_step.long()
        ]

        num_windows = indices_to_cycle_through_per_denoising_step.shape[
            1
        ]  # = num_chunks_to_denoise

        # repeat the frame indices batchwise
        indices_to_cycle_through_per_denoising_step = repeat(
            indices_to_cycle_through_per_denoising_step,
            "m num_chunks_to_denoise max_tokens -> b m num_chunks_to_denoise max_tokens",
            b=batch_size,
        )

        # merge the batch dimension with the chunk dimension to denoise all chunks in parallel
        # shape = (B, num_denoising_steps, num_chunks_to_denoise = num_windows, self.max_tokens)
        indices_to_cycle_through_per_denoising_step = rearrange(
            indices_to_cycle_through_per_denoising_step,
            "b m num_chunks_to_denoise max_tokens -> b m (num_chunks_to_denoise max_tokens)",
        )

        history_guidance = HistoryGuidance.from_config(
            config=self.cfg.tasks.prediction.history_guidance,
            timesteps=self.timesteps,
        )

        scheduling_matrix = repeat(
            scheduling_matrix, "m t -> m b t", b=xs_pred.shape[0]
        )  # shape = (num_denoising_steps, B, target_length)

        # fill in the context token's noise levels as -1 in scheduling matrix
        # not self.is_full_sequence denotes independent, per-frame noise levels
        if not self.is_full_sequence:
            scheduling_matrix = torch.where(
                context_mask[None] >= 1, -1, scheduling_matrix
            )
            scheduling_matrix = torch.where(
                context_mask[None] == -1, self.timesteps - 1, scheduling_matrix
            )

        # create chunk triplets using indices: this will result in an array of shape (B, (T//3 - 2)*self.max_tokens, ...)
        # xs_pred.shape = (B, T, self.x_shape) -> xs_pred_chunk_triplets.shape = (B, (T//3 - 2)*self.max_tokens, self.x_shape)
        # conditions.shape = (B, T, ...) -> conditions_chunk_triplets.shape = (B, (T//3 - 2)*self.max_tokens, ...)
        # scheduling_matrix.shape = (num_denoising_steps, B, T) -> from_noise_levels_chunk_triplets.shape = (num_denoising_steps, B, (T//3 - 2)*self.max_tokens)
        num_denoising_steps = scheduling_matrix.shape[0]
        scheduling_matrix = rearrange(scheduling_matrix, "m b t -> (m b) t")

        indices_repeated = repeat(
            indices_to_cycle_through_per_denoising_step,
            "b m w_times_maxt -> (m b) w_times_maxt",
            m=num_denoising_steps,
        )
        scheduling_matrix_chunk_triplets = scheduling_matrix[
            torch.arange(scheduling_matrix.shape[0], device=self.device)[:, None],
            indices_repeated,
        ]  # shape = (num_denoising_steps*B, num_windows*self.max_tokens)
        scheduling_matrix_chunk_triplets = rearrange(
            scheduling_matrix_chunk_triplets,
            "(m b) w_times_maxt -> m b w_times_maxt",
            m=num_denoising_steps,
            b=batch_size,
        )

        pbar = tqdm(
            total=num_denoising_steps - 1,
            initial=0,
            desc="GVS with no omniguidance",
            leave=False,
        )

        # iterate over denoising steps
        for m in range(num_denoising_steps - 1):
            indices = indices_to_cycle_through_per_denoising_step[:, m]

            assert (
                self.cfg.tasks.prediction.history_guidance.name == "temporal"
            ), "only temporal history guidance is supported for GVS with no omniguidance"

            # generate context_mask_chunk_triplets.shape = (B, num_windows*self.max_tokens)
            # for tokens in the left and right chunks, entry = 2     (generated history)
            # for tokens in the middle chunk, entry = 0              (to be generated)
            # for tokens in the context frames, entry = 1            (ground truth)
            context_mask_chunk_triplets = torch.zeros_like(indices)
            context_mask_chunk_triplets = rearrange(
                context_mask_chunk_triplets,
                "b (windows max_tokens) -> b windows max_tokens",
                windows=num_windows,
                max_tokens=self.max_tokens,
            )

            nfe = len(self.cfg.tasks.prediction.history_guidance.hist_weights) + 1
            context_mask_chunk_triplets = repeat(
                context_mask_chunk_triplets,
                "b windows max_tokens -> b windows nfe max_tokens",
                nfe=nfe - 1,
            ).contiguous()

            # iterate over hist_subsequences
            for i, hist_subsequence in enumerate(
                self.cfg.tasks.prediction.history_guidance.hist_subsequences
            ):
                if len(hist_subsequence) > 0:
                    # hist_subsequence is a list of int indices that we want to apply history guidance to
                    context_mask_chunk_triplets[:, :, i, hist_subsequence] = 2

            # in the first window, the leftmost chunk are context frames
            context_mask_chunk_triplets[
                :, 0, :, chunk_sizes[0] : chunk_sizes[0] + self.n_context_tokens
            ] = 1  # context frames
            context_mask_chunk_triplets = rearrange(
                context_mask_chunk_triplets,
                "b windows nfe max_tokens -> b nfe (windows max_tokens)",
            )

            # 5) coordinate the noise levels for all chunks
            from_noise_levels_chunk_triplets = scheduling_matrix_chunk_triplets[m]
            to_noise_levels_chunk_triplets = scheduling_matrix_chunk_triplets[m + 1]

            # create a backup with all context tokens unmodified
            xs_pred_prev = xs_pred.clone()

            # extract xs_pred_chunk_triplets from xs_pred
            # shape = (B, num_windows*self.max_tokens, self.x_shape)
            xs_pred_chunk_triplets = xs_pred[
                torch.arange(xs_pred.shape[0], device=self.device)[:, None], indices
            ]

            # 6) initialize the history guidance manager
            conditions_mask_chunk_triplets = None
            with history_guidance(
                context_mask_chunk_triplets
            ) as history_guidance_manager:

                nfe = history_guidance_manager.nfe
                pbar.set_postfix(NFE=nfe)
                (
                    xs_pred_chunk_triplets,
                    from_noise_levels_chunk_triplets,
                    to_noise_levels_chunk_triplets,
                    conditions_mask_chunk_triplets,
                ) = history_guidance_manager.prepare(
                    xs_pred_chunk_triplets,
                    from_noise_levels_chunk_triplets,
                    to_noise_levels_chunk_triplets,
                    replacement_fn=self.diffusion_model.q_sample,
                    replacement_only=self.is_full_sequence,
                )

                # assert that history_guidance_manager is a GeneralizedHistoryGuidanceManager
                assert isinstance(
                    history_guidance_manager, GeneralizedHistoryGuidanceManager
                ), "history_guidance_manager must be a GeneralizedHistoryGuidanceManager"

                # 7) denoise all chunks in parallel by invoking self.diffusion_model.sample_step
                # update xs_pred by DDIM or DDPM sampling
                #
                # xs_pred_chunk_triplets.shape = (B*nfe, num_windows*self.max_tokens, self.x_shape)
                # from_noise_levels_chunk_triplets.shape = (B*nfe, num_windows*self.max_tokens)
                # to_noise_levels_chunk_triplets.shape = (B*nfe, num_windows*self.max_tokens)
                # conditions_chunk_triplets.shape = (B, num_windows*self.max_tokens, self.conditions_shape) -> (B*nfe, num_windows*self.max_tokens, self.conditions_shape)
                # conditions_mask_chunk_triplets.shape = (B*nfe)
                conditions_chunk_triplets = (
                    conditions[
                        torch.arange(conditions.shape[0], device=self.device)[:, None],
                        indices,
                    ]
                    if conditions is not None
                    else None
                )
                conditions_chunk_triplets = repeat(
                    conditions_chunk_triplets, "b ... -> (b nfe) ...", nfe=nfe
                )

                # extract noise_chunk_triplets from noise
                # shape = (B, num_windows*self.max_tokens, self.x_shape)
                noise = torch.randn_like(xs_pred)
                noise = torch.clamp(noise, -self.clip_noise, self.clip_noise)
                noise_chunk_triplets = noise[
                    torch.arange(noise.shape[0], device=self.device)[:, None], indices
                ]
                noise_chunk_triplets = repeat(
                    noise_chunk_triplets, "b ... -> (b nfe) ...", nfe=nfe
                )

                # reshape chunk_triplets from (B*nfe, num_windows*self.max_tokens, ...) to (B*nfe * num_windows, self.max_tokens, ...)
                xs_pred_chunk_triplets = rearrange(
                    xs_pred_chunk_triplets,
                    "b (windows max_tokens) ... -> (b windows) max_tokens ...",
                    windows=num_windows,
                    max_tokens=self.max_tokens,
                )
                conditions_chunk_triplets = rearrange(
                    conditions_chunk_triplets,
                    "b (windows max_tokens) ... -> (b windows) max_tokens ...",
                    windows=num_windows,
                    max_tokens=self.max_tokens,
                )
                noise_chunk_triplets = rearrange(
                    noise_chunk_triplets,
                    "b (windows max_tokens) ... -> (b windows) max_tokens ...",
                    windows=num_windows,
                    max_tokens=self.max_tokens,
                )

                if self.cfg.tasks.prediction.penrose_staircase is True:
                    # hardcoded hack for penrose staircase example
                    # for the sliding window involving the final chunk to denoise (e.g. for a 120-frame long trajectory, this will be chunk 29 for a chunk-to-denoise size of 4)
                    # i.e. for both the first and last sliding windows, set the y-axis coordinate of the conditions_chunk_triplets to be all 0
                    conditions_chunk_triplets = rearrange(
                        conditions_chunk_triplets,
                        "(b windows) max_tokens ... -> b windows max_tokens ...",
                        b=batch_size * nfe,
                        windows=num_windows,
                    )
                    conditions_chunk_triplets[:, 0, :, 11] = 0
                    conditions_chunk_triplets[:, -1, :, 11] = 0
                    conditions_chunk_triplets = rearrange(
                        conditions_chunk_triplets,
                        "b windows max_tokens ... -> (b windows) max_tokens ...",
                    )

                from_noise_levels_chunk_triplets = rearrange(
                    from_noise_levels_chunk_triplets,
                    "b (windows max_tokens) ... -> (b windows) max_tokens ...",
                    windows=num_windows,
                    max_tokens=self.max_tokens,
                )
                to_noise_levels_chunk_triplets = rearrange(
                    to_noise_levels_chunk_triplets,
                    "b (windows max_tokens) ... -> (b windows) max_tokens ...",
                    windows=num_windows,
                    max_tokens=self.max_tokens,
                )

                reconstruction_guidance = self.cfg.diffusion.reconstruction_guidance
                assert (
                    reconstruction_guidance == 0
                ), "reconstruction guidance is not supported for GVS with no omniguidance"

                # indices.shape = (B, num_windows*self.max_tokens)
                indices_reshaped = rearrange(
                    indices,
                    "b (windows max_tokens) -> b windows max_tokens",
                    windows=num_windows,
                    max_tokens=self.max_tokens,
                )
                indices_to_denoise = indices_reshaped[
                    ..., chunk_sizes[0] : -chunk_sizes[-1]
                ]
                indices_to_denoise = rearrange(
                    indices_to_denoise, "b windows chunk_size -> b (windows chunk_size)"
                )

                def index_reduce_mean(placeholder, indices, source):
                    dim = 0
                    return torch.index_reduce(
                        placeholder,
                        dim,
                        indices,
                        source,
                        reduce="mean",
                        include_self=False,
                    )

                # input xs_pred_chunk_triplets.shape = (B * nfe * windows, self.max_tokens, self.x_shape)
                # output xs_pred_chunk_triplets.shape = (B * windows, self.max_tokens, self.x_shape)
                # output x0s_pred_chunk_triplets.shape = (B * windows, self.max_tokens, self.x_shape)
                xs_pred_chunk_triplets, x0s_pred_chunk_triplets = (
                    self.diffusion_model.sample_step_windows_history_guidance(
                        xs_pred_chunk_triplets,
                        from_noise_levels_chunk_triplets,
                        to_noise_levels_chunk_triplets,
                        self._process_conditions(
                            conditions_chunk_triplets,
                            from_noise_levels_chunk_triplets,
                        ),
                        (
                            repeat(
                                conditions_mask_chunk_triplets,
                                "b -> (b windows)",
                                windows=num_windows,
                            )
                            if conditions_mask_chunk_triplets is not None
                            else None
                        ),
                        num_windows=num_windows,
                        update_where_noise_level_same=self.cfg.scheduling_matrix.update_where_noise_level_same,
                        noise=noise_chunk_triplets,
                        minibatch_size=self.cfg.diffusion.minibatch_size,
                        history_guidance_manager=history_guidance_manager,
                        max_stoch=self.cfg.diffusion.max_stoch,
                    )
                )

                xs_pred_chunk_triplets = rearrange(
                    xs_pred_chunk_triplets,
                    "(b windows) max_tokens ... -> b (windows max_tokens) ...",
                    b=batch_size,
                    windows=num_windows,
                )
                x0s_pred_chunk_triplets = rearrange(
                    x0s_pred_chunk_triplets,
                    "(b windows) max_tokens ... -> b (windows max_tokens) ...",
                    b=batch_size,
                    windows=num_windows,
                )

                # 7) update xs_pred by extracting the denoised chunks from the output of the diffusion model
                # we essentially want to do the following:
                # 1) create a placeholder tensor for xs_pred of shape (B, T, self.x_shape)
                # 2) fill in the denoised chunks in the placeholder tensor at the appropriate locations:
                #    xs_pred_placeholder[indices_to_denoise_repeated] += xs_pred_chunk_triplets[center_chunk_indices]
                #    by accumulating the denoised chunks into the placeholder tensor, if there are any chunks that are denoised multiple times, the denoised chunks will be summed up
                # 3) average chunks that are denoised multiple times by dividing the placeholder tensor by the number of times each chunk is denoised
                #
                # xs_pred_chunk_triplets.shape = (B, num_windows*self.max_tokens, self.x_shape)
                # xs_pred.shape = (B, T, self.x_shape)
                xs_pred_chunk_triplets = rearrange(
                    xs_pred_chunk_triplets,
                    "b (windows max_tokens) ... -> b windows max_tokens ...",
                    windows=num_windows,
                    max_tokens=self.max_tokens,
                )
                xs_pred_chunk_to_denoise = xs_pred_chunk_triplets[
                    :, :, chunk_sizes[0] : -chunk_sizes[-1]
                ]
                xs_pred_chunk_to_denoise = rearrange(
                    xs_pred_chunk_to_denoise,
                    "b windows chunk_size ... -> b (windows chunk_size) ...",
                )

                xs_pred_placeholder = torch.zeros_like(
                    xs_pred
                )  # shape = (B, T, self.x_shape)

                xs_pred = torch.vmap(index_reduce_mean)(
                    xs_pred_placeholder, indices_to_denoise, xs_pred_chunk_to_denoise
                )  # shape = (B, T, self.x_shape)

            # only replace the tokens being generated (revert context tokens)
            xs_pred = torch.where(
                self._extend_x_dim(context_mask) == 0, xs_pred, xs_pred_prev
            )
            pbar.update(1)

        pbar.close()

        # remove padding
        xs_pred = xs_pred[:, chunk_sizes[0] : -chunk_sizes[-1]]

        return xs_pred
